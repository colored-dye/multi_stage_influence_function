{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# from trans import DecoderTransformer\n",
    "# from utils import generate_data, TextReversalDataset\n",
    "\n",
    "from modeling_gpt_neox import GPTNeoXForCausalLM\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "\n",
    "pretrained_model_path = \"/data/home/Model/Pythia/pythia-70m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model_path)\n",
    "config.use_hook = False\n",
    "model = GPTNeoXForCausalLM.from_pretrained(pretrained_model_path, config=config).to(device)\n",
    "\n",
    "print(sum([p.numel() for p in model.parameters()]))\n",
    "def count_mlp_params(model):\n",
    "    n = 0\n",
    "    for layer in model.gpt_neox.layers:\n",
    "        n += sum([p.numel() for p in layer.mlp.dense_4h_to_h.parameters()])\n",
    "    return n\n",
    "print(count_mlp_params(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"/home/Dataset/ptb_text_only\", trust_remote_code=True)\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return torch.stack([x[0] for x in batch])\n",
    "\n",
    "def get_dataloader(tokenized_split, batch_size, shuffle):\n",
    "    ds = TensorDataset(tokenized_split['input_ids'])\n",
    "    loader = DataLoader(ds, batch_size, shuffle, collate_fn=collate_fn)\n",
    "    return loader\n",
    "\n",
    "\n",
    "tokenized_ds = torch.load(\"tokenized_ds.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiSSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import get_tokenized_dataset, get_dataloader\n",
    "\n",
    "# tokenized_train_ds = get_tokenized_dataset(ds['train'], tokenizer, config.max_seq_len)\n",
    "# train_loader = get_dataloader(tokenized_train_ds, 8, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if \"dense_h_to_4h\" in n:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, jvp, grad\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "# device = \"cuda:0\"\n",
    "# model = DecoderTransformer(\n",
    "#     d_model=config.d_model,\n",
    "#     n_heads=config.n_heads,\n",
    "#     d_mlp=config.d_mlp,\n",
    "#     n_layers=config.n_layers,\n",
    "#     vocab_size=config.vocab_size,\n",
    "#     max_seq_len=config.max_seq_len,\n",
    "#     device=device,\n",
    "#     use_hook=False,\n",
    "# ).to(device)\n",
    "# model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "params = dict(model.named_parameters())\n",
    "\n",
    "ids = tokenized_ds['test']['input_ids'][0].to(device)\n",
    "targets = ids[1:]\n",
    "ids = ids[:-1].unsqueeze(0)\n",
    "\n",
    "\n",
    "def loss_with_logits(logits, targets):\n",
    "    return F.cross_entropy(logits, targets)\n",
    "\n",
    "def hvp_fn(loss_fn, params, v):\n",
    "    return jvp(grad(loss_fn), (params,), (v,))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_f_blocks(mlp_params, i):\n",
    "    for n, p in mlp_params.items():\n",
    "        params[n] = p\n",
    "    \n",
    "    out = functional_call(model, params, (i,)).logits\n",
    "    out = einops.rearrange(out, \"b s v -> (b s) v\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def loss_f_blocks(mlp_params, i, t):\n",
    "    out = out_f_blocks(mlp_params, i)\n",
    "    loss = F.cross_entropy(out, t)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def ravel_named_params(d: dict[str, torch.Tensor]):\n",
    "    names = []\n",
    "    sizes = []\n",
    "    tmp = []\n",
    "    for n, p in d.items():\n",
    "        names.append(n)\n",
    "        sizes.append(p.shape)\n",
    "        tmp.append(p.t().reshape(-1))\n",
    "    def unravel(params_flattened: torch.Tensor):\n",
    "        tmp = {}\n",
    "        pointer = 0\n",
    "        for n, s in zip(names, sizes):\n",
    "            np = s[0]*s[1] if len(s) == 2 else s[0]\n",
    "            s = (s[1], s[0]) if len(s) == 2 else s\n",
    "            tmp[n] = params_flattened[pointer:pointer+np].view(s).t().detach()\n",
    "            pointer += np\n",
    "        return tmp\n",
    "    return torch.cat(tmp).detach(), unravel\n",
    "\n",
    "\n",
    "mlp_params = {}\n",
    "for n, p in params.items():\n",
    "    if \"dense_h_to_4h\" in n:\n",
    "        mlp_params[n] = p\n",
    "\n",
    "\n",
    "grads = grad(loss_f_blocks)(mlp_params, ids, targets)\n",
    "f = lambda p: out_f_blocks(p, ids)\n",
    "Jv = jvp(f, (mlp_params,), (grads,))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.func import jvp, vjp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def sample_labels(logits: torch.Tensor):\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    if len(probs.shape) > 1:\n",
    "        return probs.multinomial(num_samples=1, replacement=True)[:, 0]\n",
    "    else:\n",
    "        return probs.multinomial(num_samples=1, replacement=True)[0]\n",
    "\n",
    "\n",
    "def gnhvp_on_sample(f, L):\n",
    "    def gnhvp_step(primals, tangents, inputs):\n",
    "        f_ = lambda p: f(p, inputs)\n",
    "        z, R_z = jvp(f_, (primals,), (tangents,))\n",
    "\n",
    "        sampled_labels = sample_labels(z)\n",
    "        L_ = lambda y: L(y, sampled_labels)\n",
    "\n",
    "        R_gz = hvp_fn(L_, z, R_z)\n",
    "        _, f_vjp = vjp(f_, primals)\n",
    "        return f_vjp(R_gz)[0]\n",
    "    return gnhvp_step\n",
    "\n",
    "\n",
    "def create_gnhvp_estimator(gnhvp_step_fn, parameters, data_loader, device):\n",
    "    ids = next(iter(data_loader))\n",
    "    ids = ids.to(device)\n",
    "    \n",
    "    def compute_fn(vec):\n",
    "        return gnhvp_step_fn(parameters, vec, ids)\n",
    "    return compute_fn\n",
    "\n",
    "\n",
    "def lissa_ignhvp(mvp: Callable,\n",
    "          vec,\n",
    "          n_iters: int,\n",
    "          damping: float,\n",
    "          alpha: float):\n",
    "    \n",
    "    vec_flattened, unravel_fn = ravel_named_params(vec)\n",
    "    ihvp = vec_flattened.clone().detach()\n",
    "\n",
    "    logs = []\n",
    "    for i in tqdm(range(n_iters)):\n",
    "        Ap = mvp(unravel_fn(ihvp))\n",
    "        Ap = ravel_named_params(Ap)[0].detach()\n",
    "        ihvp_new = vec_flattened + (1-damping*alpha)*ihvp - alpha*Ap\n",
    "        ihvp_update = torch.linalg.vector_norm(ihvp_new-ihvp)\n",
    "        logs.append(ihvp_update.item())\n",
    "\n",
    "        ihvp = ihvp_new\n",
    "\n",
    "    return ihvp, logs\n",
    "\n",
    "\n",
    "n_iters = 10\n",
    "damping = 1e-4\n",
    "alpha = 1/40\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = get_dataloader(tokenized_ds['train'], batch_size, True)\n",
    "gnhvp_step_fn = gnhvp_on_sample(out_f_blocks, loss_with_logits)\n",
    "gnhvp_estimator = create_gnhvp_estimator(gnhvp_step_fn, mlp_params, train_loader, device)\n",
    "ihvp_lissa, logs = lissa_ignhvp(gnhvp_estimator, grads, n_iters, damping, alpha)\n",
    "print(logs[-1])\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(logs)), logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "pearsonr(ihvp_lissa.cpu(), ravel_named_params(grads)[0].cpu())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EK-FAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "# model = DecoderTransformer(\n",
    "#     d_model=d_model,\n",
    "#     n_heads=n_heads,\n",
    "#     d_mlp=d_mlp,\n",
    "#     n_layers=n_layers,\n",
    "#     vocab_size=vocab_size,\n",
    "#     max_seq_len=max_seq_len,\n",
    "#     device=device,\n",
    "#     use_hook=True,\n",
    "# ).to(device)\n",
    "# model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "\n",
    "config.use_hook = False\n",
    "model = GPTNeoXForCausalLM.from_pretrained(pretrained_model_path, config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import generate_data, TextReversalDataset\n",
    "\n",
    "\n",
    "def sample_labels(logits: torch.Tensor):\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    if len(probs.shape) > 1:\n",
    "        return probs.multinomial(num_samples=1, replacement=True)[:, 0]\n",
    "    else:\n",
    "        return probs.multinomial(num_samples=1, replacement=True)[0]\n",
    "\n",
    "\n",
    "def ekfac_fit_covariance(model: DecoderTransformer,\n",
    "                         device: str,\n",
    "                         dataloader: DataLoader,\n",
    "                         n_iters: int):\n",
    "    n_blocks = len(model.blocks)\n",
    "    batch_size = dataloader.batch_size\n",
    "    \n",
    "    A = [] # Covariance of inputs\n",
    "    G = [] # Covariance of preactivation pseudo-gradients\n",
    "    for block in model.blocks:\n",
    "        mlp = block.mlp\n",
    "        out_dim, in_dim = mlp.get_dims()\n",
    "        A.append(torch.zeros((in_dim+1, in_dim+1), device=device))\n",
    "        G.append(torch.zeros((out_dim, out_dim), device=device))\n",
    "\n",
    "    logs = [\n",
    "        {\n",
    "            \"A\": [],\n",
    "            \"G\": [],\n",
    "        } for _ in range(n_blocks)\n",
    "    ]\n",
    "    for i in tqdm(range(n_iters), desc=\"Fitting covariance matrices A&G\"):\n",
    "        img, lbl = next(iter(dataloader))\n",
    "        img = img.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(img)\n",
    "        outputs = einops.rearrange(outputs, \"b s v -> (b s) v\")\n",
    "        sampled_labels = sample_labels(outputs)\n",
    "        loss = F.cross_entropy(outputs, sampled_labels, reduction=\"sum\")\n",
    "        loss.backward()\n",
    "\n",
    "        for block_idx, block in enumerate(model.blocks):\n",
    "            mlp = block.mlp\n",
    "            inputs_ = mlp.get_a_l_minus_1()\n",
    "            d_s_l = mlp.get_d_s_l()\n",
    "            seq_len = inputs_.shape[1]\n",
    "            for j in range(batch_size):\n",
    "                for k in range(seq_len):\n",
    "                    ak = inputs_[j, k:k+1, :]\n",
    "                    d_s_l_k = d_s_l[j, k:k+1, :]\n",
    "                    A[block_idx] += ak.T @ ak\n",
    "                    G[block_idx] += d_s_l_k.T @ d_s_l_k\n",
    "            \n",
    "            logs[block_idx][\"A\"].append(torch.linalg.norm(A[block_idx]/((i+1)*batch_size)).detach().cpu())\n",
    "            logs[block_idx][\"G\"].append(torch.linalg.norm(G[block_idx]/((i+1)*batch_size)).detach().cpu())\n",
    "    \n",
    "    plt.figure()\n",
    "    fig, axes = plt.subplots(2, n_blocks, figsize=(12,8))\n",
    "    for i in range(n_blocks):\n",
    "        axes[0, i].plot(range(n_iters), logs[i][\"A\"])\n",
    "        axes[0, i].set_title(f\"A[{i}]\")\n",
    "        axes[1, i].plot(range(n_iters), logs[i][\"G\"])\n",
    "        axes[1, i].set_title(f\"G[{i}]\")\n",
    "\n",
    "    QA = []\n",
    "    QG = []\n",
    "    for block_idx in range(n_blocks):\n",
    "        A[block_idx] /= n_iters*batch_size\n",
    "        G[block_idx] /= n_iters*batch_size\n",
    "\n",
    "        _, qa = torch.linalg.eigh(A[block_idx])\n",
    "        _, qg = torch.linalg.eigh(G[block_idx])\n",
    "        QA.append(qa.detach())\n",
    "        QG.append(qg.detach())\n",
    "    return QA, QG\n",
    "\n",
    "\n",
    "n_iters = 5000\n",
    "damping = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "train_data, train_targets, test_data, test_targets = generate_data(0, trainset_size+testset_size, testset_size, max_seq_len)\n",
    "trainset = TextReversalDataset(train_data, train_targets, max_seq_len)\n",
    "testset = TextReversalDataset(test_data, test_targets, max_seq_len)\n",
    "train_loader = DataLoader(trainset, batch_size, True)\n",
    "\n",
    "QA, QG = ekfac_fit_covariance(model, device, train_loader, n_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    The vectorization process stacks columns of a matrix into a single vector.\n",
    "\n",
    "    For a matrix x of shape (M, N),\n",
    "    `vectorize(x) = [x[:,0]^T, x[:, 1]^T, ..., x[:, N-1]^T]^T`.\n",
    "    \"\"\"\n",
    "    # rows, cols = x.shape\n",
    "    # ans = torch.cat([x[:, i] for i in range(cols)], dim=0)\n",
    "    return x.t().reshape(-1)\n",
    "\n",
    "def unvectorize(x: torch.Tensor, rows, cols):\n",
    "    # ans = torch.stack([x[i*rows:i*rows+rows] for i in range(cols)], dim=1)\n",
    "    return x.reshape(cols, rows).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def ekfac_fit_diagonal(model: DecoderTransformer,\n",
    "                       device: str,\n",
    "                       dataloader: DataLoader,\n",
    "                       n_iters: int,\n",
    "                       QA: list[torch.Tensor],\n",
    "                       QG: list[torch.Tensor]):\n",
    "    n_blocks = len(model.blocks)\n",
    "    batch_size = dataloader.batch_size\n",
    "\n",
    "    Lambda = []\n",
    "    for block in model.blocks:\n",
    "        mlp = block.mlp\n",
    "        out_dim, in_dim = mlp.get_dims()\n",
    "        Lambda.append(torch.zeros(((in_dim+1)*out_dim), device=device))\n",
    "    \n",
    "    for i in tqdm(range(n_iters), desc=\"Fitting diagonal\"):\n",
    "        img, lbl = next(iter(dataloader))\n",
    "        img = img.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(img)\n",
    "        outputs = einops.rearrange(outputs, \"b s v -> (b s) v\")\n",
    "        sampled_labels = sample_labels(outputs)\n",
    "        loss = F.cross_entropy(outputs, sampled_labels, reduction=\"sum\")\n",
    "        loss.backward()\n",
    "\n",
    "        for block_idx, block in enumerate(model.blocks):\n",
    "            mlp = block.mlp\n",
    "            dw = mlp.get_d_w_l()\n",
    "            result = QG[block_idx].T @ dw @ QA[block_idx]\n",
    "            result = vectorize(result)\n",
    "            Lambda[block_idx] += result.pow(2)\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "        Lambda[i] /= n_iters*batch_size\n",
    "    return Lambda\n",
    "\n",
    "train_data, train_targets, test_data, test_targets = generate_data(0, trainset_size+testset_size, testset_size, max_seq_len)\n",
    "trainset = TextReversalDataset(train_data, train_targets, max_seq_len)\n",
    "testset = TextReversalDataset(test_data, test_targets, max_seq_len)\n",
    "train_loader = DataLoader(trainset, batch_size, True)\n",
    "\n",
    "Lambda = ekfac_fit_diagonal(model, device, train_loader, n_iters, QA, QG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(QA, \"QA.pt\")\n",
    "torch.save(QG, \"QG.pt\")\n",
    "torch.save(Lambda, \"Lambda.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = torch.load(\"QA.pt\", map_location=device)\n",
    "QG = torch.load(\"QG.pt\", map_location=device)\n",
    "Lambda = torch.load(\"Lambda.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_grads(net,\n",
    "                  inputs: torch.Tensor,\n",
    "                  targets: torch.Tensor):\n",
    "    net.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    outputs = einops.rearrange(outputs, \"b s v -> (b s) v\")\n",
    "    loss = F.cross_entropy(outputs, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = []\n",
    "    for block in net.blocks:\n",
    "        mlp = block.mlp\n",
    "        grads.append(mlp.get_d_w_l())\n",
    "    return grads\n",
    "\n",
    "\n",
    "def ekfac_ihvp_single_block(qa: torch.Tensor,\n",
    "                            qg: torch.Tensor,\n",
    "                            diagonal: torch.Tensor,\n",
    "                            damping: float,\n",
    "                            v: torch.Tensor):\n",
    "    qg_v_qa = qg.T @ v @ qa\n",
    "    diagonal += damping\n",
    "    diagonal = unvectorize(diagonal, v.shape[0], v.shape[1])\n",
    "    result = qg_v_qa / diagonal\n",
    "    ihvp = qg @ result @ qa.T\n",
    "    return ihvp\n",
    "\n",
    "\n",
    "def ekfac_ihvp(QA: list[torch.Tensor],\n",
    "               QG: list[torch.Tensor],\n",
    "               Lambda: list[torch.Tensor],\n",
    "               damping: float,\n",
    "               vec: list[torch.Tensor]):\n",
    "    ihvps = []\n",
    "    for qa, qg, diagonal, v in zip(QA, QG, Lambda, vec):\n",
    "        ihvp = ekfac_ihvp_single_block(qa, qg, diagonal, damping, v)\n",
    "        ihvp = vectorize(ihvp)\n",
    "        ihvps.append(ihvp)\n",
    "    return torch.cat(ihvps)\n",
    "\n",
    "\n",
    "img, lbl = trainset[0]\n",
    "img, lbl = img.to(device), lbl.to(device)\n",
    "vec = example_grads(model, img, lbl)\n",
    "ihvp_ekfac = ekfac_ihvp(QA, QG, Lambda, damping, vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "pearsonr(ihvp_lissa.cpu(), ihvp_ekfac.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "pearsonr(ihvp_lissa.cpu(), ravel_named_params(grads)[0].cpu())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, jvp, grad\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "params = dict(model.named_parameters())\n",
    "\n",
    "\n",
    "def out_f_all(params, i):\n",
    "    out = functional_call(model, params, (i,)).logits\n",
    "    out = einops.rearrange(out, \"b s v -> (b s) v\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def loss_with_logits(logits, targets):\n",
    "    return F.cross_entropy(logits, targets)\n",
    "\n",
    "def hvp_fn(loss_fn, params, v):\n",
    "    return jvp(grad(loss_fn), (params,), (v,))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.func import jvp, grad, vjp\n",
    "\n",
    "\n",
    "def sample_labels(logits: torch.Tensor):\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    if len(probs.shape) > 1:\n",
    "        return probs.multinomial(num_samples=1, replacement=True)[:, 0]\n",
    "    else:\n",
    "        return probs.multinomial(num_samples=1, replacement=True)[0]\n",
    "\n",
    "\n",
    "def gnhvp_on_sample(f, L):\n",
    "    def gnhvp_step(primals, tangents, batch):\n",
    "        f_ = lambda p: f(p, batch)\n",
    "        z, R_z = jvp(f_, (primals,), (tangents,))\n",
    "\n",
    "        sampled_labels = sample_labels(z)\n",
    "        L_ = lambda y: L(y, sampled_labels)\n",
    "\n",
    "        R_gz = hvp_fn(L_, z, R_z)\n",
    "        _, f_vjp = vjp(f_, primals)\n",
    "        return f_vjp(R_gz)[0]\n",
    "    return gnhvp_step\n",
    "\n",
    "\n",
    "def create_gnhvp_estimator(gnhvp_step_fn, parameters, data_loader, device):\n",
    "    ids = next(iter(data_loader))\n",
    "    ids = ids.to(device)\n",
    "    \n",
    "    def compute_fn(vec):\n",
    "        return gnhvp_step_fn(parameters, vec, ids)\n",
    "    return compute_fn\n",
    "\n",
    "\n",
    "def conjugate_gradient(mvp_fn, b, damping, max_iter):\n",
    "    b_flattened, unravel_fn = ravel_named_params(b)\n",
    "\n",
    "    x = torch.zeros_like(b_flattened)\n",
    "    r = b_flattened.clone().detach()\n",
    "    p = r.clone().detach()\n",
    "    rdotr = r.dot(r)\n",
    "\n",
    "    logs = []\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        Ap = mvp_fn(unravel_fn(p))\n",
    "        Ap = ravel_named_params(Ap)[0]\n",
    "        Ap += damping * p\n",
    "        v = rdotr / (p.dot(Ap)+1e-11)\n",
    "        x_new = x - v * p\n",
    "        logs.append(torch.linalg.vector_norm(x - x_new).item())\n",
    "        x = x_new\n",
    "        r -= v * Ap\n",
    "        newrdotr = r.dot(r)\n",
    "        mu = newrdotr / rdotr\n",
    "        p = r + mu * p\n",
    "        rdotr = newrdotr\n",
    "    return x, logs\n",
    "\n",
    "\n",
    "n_iters = 100\n",
    "damping = 1e-4\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = get_dataloader(tokenized_ds['train'], batch_size, True)\n",
    "# gnhvp_step_fn = gnhvp_on_sample(out_f_blocks, loss_with_logits)\n",
    "# gnhvp_estimator = create_gnhvp_estimator(gnhvp_step_fn, mlp_params, train_loader, device)\n",
    "\n",
    "gnhvp_step_fn = gnhvp_on_sample(out_f_all, loss_with_logits)\n",
    "gnhvp_estimator = create_gnhvp_estimator(gnhvp_step_fn, params, train_loader, device)\n",
    "\n",
    "ihvp_cg, logs = conjugate_gradient(gnhvp_estimator, grads, damping, n_iters)\n",
    "print(logs[-1])\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(logs)), logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "pearsonr(ihvp_lissa.cpu(), ihvp_cg.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "pearsonr(ihvp_ekfac.cpu(), ihvp_cg.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "pearsonr(ravel_named_params(grads)[0].cpu(), ihvp_cg.cpu())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "\n",
    "# device = \"cuda:3\"\n",
    "device = \"cuda\"\n",
    "pretrained_model_path = \"/data/home/Model/Pythia/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from modeling_gpt_neox import GPTNeoXForCausalLM\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig\n",
    ")\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path, padding_side='left')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"config.json\")\n",
    "config.use_hook = False\n",
    "_model = GPTNeoXForCausalLM(config=config)\n",
    "# _model.load_state_dict(torch.load(\"model.ckpt\", map_location=device))\n",
    "_model = _model.to(device)\n",
    "model = CustomModel(_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "ds = load_dataset(\"/home/Dataset/ptb_text_only\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"choices_candidates.pkl\", \"rb\") as fp:\n",
    "    choices_candidates = pickle.load(fp)\n",
    "\n",
    "with open(\"choices_queries.pkl\", \"rb\") as fp:\n",
    "    choices_queries = pickle.load(fp)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "choices_candidates = np.array(choices_candidates)\n",
    "choices_candidates.size, len(set(choices_candidates.ravel().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_candidates = ds['train'].select(choices_candidates[0])\n",
    "ds_queries = ds['test'].select([choices_queries[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "def tokenize_and_pad(examples):\n",
    "    return tokenizer(examples['sentence'], max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "\n",
    "ds['train'] = ds_candidates\n",
    "ds['test'] = ds_queries\n",
    "tokenized_ds = ds.map(tokenize_and_pad, batched=True, remove_columns=['sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from trak import TRAKer\n",
    "from trak.modelout_functions import AbstractModelOutput\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "class CustomModelOutput(AbstractModelOutput):\n",
    "    @staticmethod\n",
    "    def get_output(model, weights, buffers, input_ids, attention_mask, label):\n",
    "        # kw_inputs = {\n",
    "        #     \"input_ids\": input_ids,\n",
    "        #     \"attention_mask\": attention_mask,\n",
    "        # }\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "        attention_mask = attention_mask.unsqueeze(0)\n",
    "        outputs = torch.func.functional_call(model, (weights, buffers), args=(input_ids, attention_mask))\n",
    "        logits = outputs.logits\n",
    "        logits = logits.reshape(-1, logits.size(-1))\n",
    "        label = label.reshape(-1)\n",
    "        loss = F.cross_entropy(logits, label, reduction=\"sum\")\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def get_out_to_loss_grad(model, weights, buffers, batch: Iterable[Tensor]) -> Tensor:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        # kw_inputs = {\n",
    "        #     \"input_ids\": input_ids,\n",
    "        #     \"attention_mask\": attention_mask,\n",
    "        # }\n",
    "        outputs = torch.func.functional_call(model, (weights, buffers), args=(input_ids, attention_mask))\n",
    "        logits = outputs.logits\n",
    "        batch_size = logits.size(0)\n",
    "        logits = logits.reshape(-1, logits.size(-1))\n",
    "        labels = labels.reshape(-1)\n",
    "        out_grads = torch.func.grad(lambda logits, labels: F.cross_entropy(logits, labels, reduction='sum'))(logits, labels)\n",
    "        # out_grads = torch.sum(out_grads, dim=1, keepdim=True).clone().detach()\n",
    "        out_grads = out_grads.reshape(batch_size, -1)\n",
    "        out_grads = torch.sum(out_grads, dim=1, keepdim=True)\n",
    "        # print(out_grads.shape)\n",
    "        return out_grads\n",
    "\n",
    "\n",
    "traker = TRAKer(model=model,\n",
    "                task=CustomModelOutput,\n",
    "                train_set_size=len(ds['train']),\n",
    "                device=device,\n",
    "                proj_dim=4096,\n",
    "                lambda_reg=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "ckpt_dir = \"checkpoints/\"\n",
    "n_ckpts = 10\n",
    "\n",
    "batch_size = 48\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return (\n",
    "        torch.stack(([torch.tensor(x['input_ids'][:-1]) for x in batch])),\n",
    "        torch.stack(([torch.tensor(x['attention_mask'][:-1]) for x in batch])),\n",
    "        torch.stack(([torch.tensor(x['input_ids'][1:]) for x in batch])),\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(tokenized_ds['train'], batch_size, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "time_featurize = 0\n",
    "for model_id in tqdm(range(n_ckpts), desc=\"Checkpoints\"):\n",
    "    ckpt = torch.load(os.path.join(ckpt_dir, f\"{model_id}.ckpt\"), map_location=device)\n",
    "    model.model.load_state_dict(ckpt)\n",
    "    traker.load_checkpoint(model.state_dict(), model_id=model_id)\n",
    "\n",
    "    start = time.time()\n",
    "    for batch in tqdm(train_loader, desc=f\"Ckpt [{model_id}]\"):\n",
    "        batch = [x.to(device) for x in batch]\n",
    "        traker.featurize(batch=batch, num_samples=batch[0].shape[0])\n",
    "    end = time.time()\n",
    "    time_featurize += end-start\n",
    "\n",
    "import logging\n",
    "loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "for logger in loggers:\n",
    "    print(logger.name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "start = time.time()\n",
    "traker.finalize_features()\n",
    "end = time.time()\n",
    "time_featurize += end-start\n",
    "print(f\"Time on featurizing: {time_featurize:.3f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(tokenized_ds['test'], batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "time_score = 0\n",
    "for model_id in range(n_ckpts):\n",
    "    ckpt = torch.load(os.path.join(ckpt_dir, f\"{model_id}.pkl\"), map_location=device)\n",
    "    model.model.load_state_dict(ckpt)\n",
    "\n",
    "    start = time.time()\n",
    "    traker.start_scoring_checkpoint(exp_name=\"test\",\n",
    "                                    checkpoint=model.state_dict(),\n",
    "                                    model_id=model_id,\n",
    "                                    num_targets=len(ds['test']))\n",
    "\n",
    "    for batch in tqdm(test_loader):\n",
    "        batch = [x.to(device) for x in batch]\n",
    "        traker.score(batch, num_samples=batch[0].shape[0])\n",
    "    end = time.time()\n",
    "    time_score += end-start\n",
    "\n",
    "start = time.time()\n",
    "scores = traker.finalize_scores(exp_name=\"test\")\n",
    "end = time.time()\n",
    "time_score += end-start\n",
    "print(f\"Time on scoring: {end-start:.3f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"infls/trak\", exist_ok=True)\n",
    "\n",
    "n_queries = len(choices_queries)\n",
    "n_candidates = len(choices_candidates[0])\n",
    "\n",
    "for i in range(n_queries):\n",
    "    infls = scores[:, i].tolist()\n",
    "    with open(f\"infls/trak/{i}.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(infls, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHA/MLP parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_gpt_neox import GPTNeoXForCausalLM\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig\n",
    ")\n",
    "import torch\n",
    "config = AutoConfig.from_pretrained(\"config.json\")\n",
    "config.use_hook = False\n",
    "model = GPTNeoXForCausalLM(config=config)\n",
    "model.load_state_dict(torch.load(\"model.ckpt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "n_mha_params = 0\n",
    "n_mlp_params = 0\n",
    "for n, p in model.named_parameters():\n",
    "    print(n)\n",
    "    total += p.numel()\n",
    "    if \"query_key_value\" in n:\n",
    "        n_mha_params += p.numel()\n",
    "    elif \"dense\" in n:\n",
    "        n_mlp_params += p.numel()\n",
    "total, n_mha_params, n_mlp_params, n_mha_params/(n_mha_params+n_mlp_params), n_mlp_params/(n_mha_params+n_mlp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample influence estimation targets and candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from utils import generate_data, TextReversalDataset\n",
    "from utils import get_tokenized_dataset, get_dataloader\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def sample_uniform(seed: int, n_total: int, n_choices: int):\n",
    "    np.random.seed(seed)\n",
    "    choices = np.random.choice(np.arange(n_total), n_choices, replace=False)\n",
    "    return choices.tolist()\n",
    "\n",
    "n_queries = 10\n",
    "n_candidates = 500\n",
    "seed = 42\n",
    "\n",
    "# trainset_size = 100_000\n",
    "# testset_size = 1_000\n",
    "\n",
    "ds = load_dataset(\"/home/Dataset/ptb_text_only\", trust_remote_code=True)\n",
    "# train_data, train_targets, test_data, test_targets = generate_data(0, trainset_size+testset_size, testset_size, max_seq_len)\n",
    "# trainset = TextReversalDataset(train_data, train_targets, max_seq_len)\n",
    "# testset = TextReversalDataset(test_data, test_targets, max_seq_len)\n",
    "\n",
    "choices_queries = sample_uniform(seed, len(ds['test']), n_queries)\n",
    "\n",
    "choices_candidates = []\n",
    "for i in range(n_queries):\n",
    "    choices_candidates.append(sample_uniform(seed+1+i, len(ds['train']), n_candidates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"choices_candidates.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(choices_candidates, fp)\n",
    "\n",
    "with open(\"choices_queries.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(choices_queries, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def read_infls(infl_dir: str):\n",
    "    infls = []\n",
    "    for f in sorted(os.listdir(infl_dir)):\n",
    "        with open(os.path.join(infl_dir, f), \"rb\") as fp:\n",
    "            infl = pickle.load(fp)\n",
    "        infls.append(infl)\n",
    "    return infls\n",
    "\n",
    "\n",
    "infls = {}\n",
    "algorithms = sorted(os.listdir(\"infls\"))\n",
    "for alg in algorithms:\n",
    "    infls[alg] = read_infls(f\"infls/{alg}\")\n",
    "    print(f\"{alg}: {len(infls[alg])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fix_tensor_type(infls: dict[str, list]):\n",
    "    infls_fixed = []\n",
    "    for infl in infls:\n",
    "        if str(infl[0].__class__) == \"<class 'torch.Tensor'>\":\n",
    "            infls_fixed.append(np.array([i.detach().cpu() for i in infl]))\n",
    "        else:\n",
    "            infls_fixed.append(np.array(infl))\n",
    "    return infls_fixed\n",
    "\n",
    "for alg in algorithms:\n",
    "    infls[alg] = fix_tensor_type(infls[alg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(infls['trak'])):\n",
    "    infls['trak'][i] = infls['trak'][i][i*500:(i+1)*500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "baseline = \"cg-all\"\n",
    "\n",
    "results = {}\n",
    "for alg in algorithms:\n",
    "    if alg != baseline:\n",
    "        ps = []\n",
    "        for infls1, infls2 in zip(infls[alg], infls[baseline]):\n",
    "            ps.append(pearsonr(infls1, infls2).statistic)\n",
    "        std = np.std(ps)\n",
    "        ps = np.mean(ps)\n",
    "        \n",
    "        results[alg] = ps, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "table_content = [(n, p) for n, p in results.items()]\n",
    "print(tabulate(table_content, headers=[\"Algorithm\", \"Pearson correlation\"], tablefmt=\"pipe\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted = dict(sorted(results.items(), key=lambda item: np.abs(item[1][0]), reverse=True))\n",
    "table_content = [(n, p) for n, p in results_sorted.items()]\n",
    "print(tabulate(table_content, headers=[\"Algorithm\", \"Pearson correlation\"], tablefmt=\"pipe\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytablewriter import MarkdownTableWriter\n",
    "from pytablewriter.style import Cell, Style\n",
    "\n",
    "results_sorted = dict(sorted(results.items(), key=lambda item: np.abs(item[1]), reverse=True))\n",
    "table_content = [(n, f\"{abs(p):.6f}\") for n, p in results_sorted.items()]\n",
    "\n",
    "\n",
    "def style_filter(cell: Cell, **kwargs):\n",
    "    if cell.is_header_row():\n",
    "        return None\n",
    "    \n",
    "    style = Style(align=\"center\")\n",
    "\n",
    "    if cell.row == 0 and cell.col == 1:\n",
    "        style.update(font_weight=\"bold\")\n",
    "    elif cell.row == 1 and cell.col == 1:\n",
    "        style.update(font_style=\"italic\")\n",
    "    \n",
    "    return style\n",
    "\n",
    "\n",
    "table_writer = MarkdownTableWriter(\n",
    "    table_name=\"Scalability validation (MLP)\",\n",
    "    headers=[\"Algorithm\", \"Pearson correlation\"],\n",
    "    value_matrix=table_content,\n",
    "    flavor=\"github\",\n",
    ")\n",
    "table_writer.add_style_filter(style_filter)\n",
    "table_writer.write_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "baseline = \"cg-all\"\n",
    "\n",
    "results = {}\n",
    "for alg in algorithms:\n",
    "    if alg != baseline:\n",
    "        ps = []\n",
    "        for infls1, infls2 in zip(infls[alg], infls[baseline]):\n",
    "            ps.append(spearmanr(infls1, infls2).statistic)\n",
    "        std = np.std(ps)\n",
    "        ps = np.mean(ps)\n",
    "        results[alg] = ps, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "table_content = [(n, p) for n, p in results.items()]\n",
    "print(tabulate(table_content, headers=[\"Algorithm\", \"Spearman correlation\"], tablefmt=\"pipe\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted = dict(sorted(results.items(), key=lambda item: np.abs(item[1][0]), reverse=True))\n",
    "table_content = [(n, p) for n, p in results_sorted.items()]\n",
    "print(tabulate(table_content, headers=[\"Algorithm\", \"Spearman correlation\"], tablefmt=\"pipe\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytablewriter import MarkdownTableWriter\n",
    "from pytablewriter.style import Cell, Style\n",
    "\n",
    "results_sorted = dict(sorted(results.items(), key=lambda item: np.abs(item[1]), reverse=True))\n",
    "table_content = [(n, f\"{abs(p):.6f}\") for n, p in results_sorted.items()]\n",
    "\n",
    "\n",
    "def style_filter(cell: Cell, **kwargs):\n",
    "    if cell.is_header_row():\n",
    "        return None\n",
    "    \n",
    "    style = Style(align=\"center\")\n",
    "\n",
    "    if cell.row == 0 and cell.col == 1:\n",
    "        style.update(font_weight=\"bold\")\n",
    "    elif cell.row == 1 and cell.col == 1:\n",
    "        style.update(font_style=\"italic\")\n",
    "    \n",
    "    return style\n",
    "\n",
    "\n",
    "table_writer = MarkdownTableWriter(\n",
    "    table_name=\"Scalability validation (MLP)\",\n",
    "    headers=[\"Algorithm\", \"Spearman correlation\"],\n",
    "    value_matrix=table_content,\n",
    "    flavor=\"github\",\n",
    ")\n",
    "table_writer.add_style_filter(style_filter)\n",
    "table_writer.write_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/Dataset/wikitext-103/wiki.valid.tokens\", \"r\") as fp:\n",
    "    valid = fp.read()\n",
    "valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set_theme(context='paper', style='whitegrid')\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=BIGGER_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "with open(\"results.json\", \"r\") as fp:\n",
    "    results = json.load(fp)\n",
    "\n",
    "results = [r for r in results if \"all\" in r['alg'] or \"ekfac\" in r['alg']]\n",
    "\n",
    "for i in range(len(results)):\n",
    "    results[i][\"overhead\"] /= 3600\n",
    "    results[i][\"job\"] /= 3600*10\n",
    "    results[i][\"time\"] = results[i][\"overhead\"] + results[i][\"job\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = []\n",
    "for rec in results:\n",
    "    ax = plt.scatter(rec['time'], rec['pc'], color=rec['color'], marker=rec['marker'], s=64)\n",
    "    axes.append(ax)\n",
    "    if 'ekfac-all' == rec['alg'] or 'ekfac-mlp' == rec['alg'] or 'arnoldi' in rec['alg'] or 'trak' in rec['alg']:\n",
    "        ax = plt.scatter(rec['job'], rec['pc'], color=rec['color'], marker=rec['marker'], s=64, facecolors='none')\n",
    "        axes.append(ax)\n",
    "\n",
    "\n",
    "groups = [4, 2, 2, 4, 1, 1, 2, 2]\n",
    "axes_legend = []\n",
    "p = 0\n",
    "for group in groups:\n",
    "    if group > 1:\n",
    "        axes_legend.append(tuple(axes[p:p+group]))\n",
    "    else:\n",
    "        axes_legend.append(axes[p])\n",
    "    p += group\n",
    "plt.legend(axes_legend,\n",
    "    [\n",
    "    \"CG-all\",\n",
    "    \"Ours\",\n",
    "    \"Ours-mlp\",\n",
    "    \"LiSSA\",\n",
    "    \"Arnoldi\",\n",
    "    \"GDP\",\n",
    "    \"CKA\",\n",
    "    \"Arnoldi\",\n",
    "    \"TRAK\",\n",
    "    ],\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "    loc=9,\n",
    "    bbox_to_anchor=(1.2, 0.663)\n",
    "    )\n",
    "plt.xlabel(\"Time (h)\")\n",
    "# plt.xlabel(\"Time (s)\")\n",
    "plt.xticks(np.arange(0, 21, 2))\n",
    "plt.ylabel(\"Pearson correlation\")\n",
    "plt.ylim(top=1.02)\n",
    "plt.title(\"Ground truth: CG (All parameters)\")\n",
    "plt.xscale('log')\n",
    "\n",
    "import os\n",
    "figures_dir = \"paper-figures/\"\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "figure_name = \"pearson_corr_transformer_all.pdf\"\n",
    "plt.savefig(os.path.join(figures_dir, figure_name), format=\"pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set_theme(context='paper', style='whitegrid')\n",
    "import numpy as np\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=BIGGER_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "with open(\"results.json\", \"r\") as fp:\n",
    "    results = json.load(fp)\n",
    "\n",
    "results = [r for r in results if \"all\" in r['alg'] or \"ekfac\" in r['alg']]\n",
    "\n",
    "for i in range(len(results)):\n",
    "    results[i][\"overhead\"] /= 3600\n",
    "    results[i][\"job\"] /= 3600*10\n",
    "    results[i][\"time\"] = results[i][\"overhead\"] + results[i][\"job\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = []\n",
    "for rec in results:\n",
    "    ax = plt.scatter(rec['time'], rec['sc'], color=rec['color'], marker=rec['marker'], s=64)\n",
    "    axes.append(ax)\n",
    "    if 'ekfac-all' == rec['alg'] or 'ekfac-mlp' == rec['alg'] or 'arnoldi' in rec['alg'] or 'trak' in rec['alg']:\n",
    "        ax = plt.scatter(rec['job'], rec['sc'], color=rec['color'], marker=rec['marker'], s=64, facecolors='none')\n",
    "        axes.append(ax)\n",
    "\n",
    "\n",
    "groups = [4, 2, 2, 4, 1, 1, 2, 2]\n",
    "axes_legend = []\n",
    "p = 0\n",
    "for group in groups:\n",
    "    if group > 1:\n",
    "        axes_legend.append(tuple(axes[p:p+group]))\n",
    "    else:\n",
    "        axes_legend.append(axes[p])\n",
    "    p += group\n",
    "plt.legend(axes_legend,\n",
    "    [\n",
    "    \"CG-all\",\n",
    "    \"Ours\",\n",
    "    \"Ours-mlp\",\n",
    "    \"LiSSA\",\n",
    "    \"GDP\",\n",
    "    \"CKA\",\n",
    "    \"Arnoldi\",\n",
    "    \"TRAK\",\n",
    "    ],\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "    loc=9,\n",
    "    bbox_to_anchor=(1.2, 0.663)\n",
    "    )\n",
    "plt.xlabel(\"Time (h)\")\n",
    "# plt.xlabel(\"Time (s)\")\n",
    "plt.xticks(np.arange(0, 21, 2))\n",
    "plt.ylabel(\"Spearman correlation\\n($\\\\rightarrow$Better)\")\n",
    "plt.ylim(top=1.02)\n",
    "plt.title(\"($\\leftarrow$Better)\")\n",
    "plt.xscale('log')\n",
    "\n",
    "import os\n",
    "figures_dir = \"paper-figures/\"\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "figure_name = \"spearman_corr_transformer_all.pdf\"\n",
    "plt.savefig(os.path.join(figures_dir, figure_name), format=\"pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time trend with scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set_theme(context='paper', style='whitegrid')\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=BIGGER_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "n_params = [268800, 1861632, 10014720, 56494080, 108876800, 217845760]\n",
    "overhead = [90.72, 93.4, 134, 820.82, 1376.82, 5201.11]\n",
    "job = [7.62, 7.65, 9.46, 16.95, 47.54, 52.49]\n",
    "pd_table = dict([(p, t) for p, t in zip(n_params, overhead)])\n",
    "\n",
    "time = [o + j for o, j in zip(overhead, job)]\n",
    "\n",
    "plt.plot(n_params, overhead, c='C0')\n",
    "plt.plot(n_params, time, c='C1')\n",
    "plt.scatter(n_params, overhead, c='C0')\n",
    "plt.scatter(n_params, time, c='C1')\n",
    "\n",
    "def line(x, a):\n",
    "    return a * x\n",
    "\n",
    "from scipy import optimize\n",
    "parameters, covariance = optimize.curve_fit(line, n_params, overhead)\n",
    "import numpy as np\n",
    "x = np.arange(0, n_params[-1], step=1e4)\n",
    "fitted = line(x, parameters[0])\n",
    "plt.plot(x, fitted, 'r--')\n",
    "plt.xlabel(\"Number of parameters\")\n",
    "plt.ylabel(\"Run time (s)\")\n",
    "\n",
    "import os\n",
    "figures_dir = \"paper-figures/\"\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "figure_name = \"infl-time.pdf\"\n",
    "plt.savefig(os.path.join(figures_dir, figure_name), format=\"pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
